###	Takes 3 inputs of 0 or 1 #, outputs a single number, 0 or 1. 
###	Simple rule for output is that a 1 in first input makes output 1 also

import numpy as np 

def sigmoid(x):
	'''return output [0,1]'''
	return 1 / ( 1 + np.exp(-x))

def sigmoid_derivative(x):
	return x * (1 - x)

training_inputs = np.array([[0,0,1],
							[1,1,1],
							[1,0,1],
							[0,1,1]])
training_outputs = np.array([[0,1,1,0]]).T #.T transposed into column vector 

np.random.seed(1) #only used to ensure same random numbers as tutorial

synaptic_weights = 2 * np.random.random((3, 1)) - 1 #to get from -1 to 1 

print('Random weights are:')
print(synaptic_weights)

for iteration in range(20000):

	input_layer = training_inputs

	outputs = sigmoid(np.dot(input_layer, synaptic_weights ))  # sigmoid of ( weighted average of input_layer )

	error = training_outputs - outputs
	#full adjustment is input * error * sigmoid_derivaitve

	adjustments = error * sigmoid_derivative(outputs)

	synaptic_weights += np.dot(input_layer.T, adjustments)

print('Synaptic weights after training')
print(synaptic_weights)


print('Output after training:')
print(outputs)
